# Site
repository: sproogen/resume-theme
favicon: images/favicon.ico

# Content configuration version
version: 2

# Personal info
name: Hyundong Jin 
title: Curriculum Vitae
email: jude0316@cau.ac.kr
website: https://vllab.cau.ac.kr/members/

# Dark Mode (true/false/never)
darkmode: false

# Social links
# twitter_username: facespics
github_username:  Jin0316
# stackoverflow_username: "00000001"
# dribbble_username: jekyll
# facebook_username: jekyll
# flickr_username: jekyll
# instagram_username: jameswgrant
# linkedin_username: jameswgrant
# xing_username: jekyll
# pinterest_username: jekyll
# youtube_username: globalmtb
# googleplus_username: +jekyll
# orcid_username: 0000-0000-0000-0000

# Additional icon links
# additional_links:
# - title: itsgoingto.be
#   icon: fas fa-globe
#   url: https://www.itsgoingto.be
# - title: another link
#   icon: font awesome brand icon name (eg. fab fa-twitter) (https://fontawesome.com/icons?d=gallery&m=free)
#   url: Link url (eg. https://google.com)

# Google Analytics and Tag Manager
# Using more than one of these may cause issues with reporting
# gtm: "GTM-0000000"
# gtag: "UA-00000000-0"
# google_analytics: "UA-00000000-0"

# About Section
about_title: About Me
about_profile_image: images/profile_hyundongjin.png

about_content: | # this will include new lines to allow paragraphs
  Hyundong Jin received the B.S degree in Eletrical and Electronics Engineering from Chung-Ang Univrsity, in 2020, 
  and the M.S degree in School of Computer Science and Engineering from Chung-Ang University in 2022. 
  
  He is currently Ph.D candidate in School of Computer Science of Chung-Ang University. 
 
  His research interests include deep learning, machine learning, continual learning, and computer vision.

  He is most skilled in: Continual Learning.

content:

  - title: Education  # Title for the section
    layout: list # Type of content section (list/text)
    content:     
      - layout: left
        title: Chung-Ang University
        caption: Mar. 2022 - present
        description: | # this will include new lines to allow paragraphs
            Ph.D. of Computer Science Engineering (advisor: Eunwoo Kim)
            
      - layout: left
        title: Chung-Ang University
        caption: Mar. 2020 - Feb.2022
        description: | # this will include new lines to allow paragraphs
            Master of Computer Science Engineering (advisor: Eunwoo Kim)
            
      - layout: left
        title: Chung-Ang University
        caption: Mar. 2015 - Feb.2020
        description: | # this will include new lines to allow paragraphs
            Bachelor of Electrical and Electronics Engineering.

##### Paper #####

  - title: Published Papers  # Title for the section
    layout: list # Type of content section (list/text)
    content:
      - layout: left
        title: ECCV 2022
        link: https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710518.pdf
        link_text: [Link to Paper] 
        sub_title: Top-tier Conference in Computer Vision & Machine Learning
        caption: 
        quote: >
          Helpful or Harmful: Inter-Task Association in Continual Learning, **Hyundong Jin** and Eunwoo Kim
        description: | # this will include new lines to allow paragraphs
            Abstract: When optimizing sequentially incoming tasks, deep neural networks generally suffer from catastrophic forgetting due to their lack of ability to maintain knowledge from old tasks. 
                    This may lead to a significant performance drop of the previously learned tasks. To alleviate this problem, studies on continual learning have been conducted as a countermeasure. 
                    Nevertheless, it suffers from an increase in computational cost due to the expansion of the network size or a change in knowledge that is favorably linked to previous tasks. 
                    In this work, we propose a novel approach to differentiate helpful and harmful information for old tasks using a model search to learn a current task effectively. 
                    Given a new task, the proposed method discovers an underlying association knowledge from old tasks, which can provide additional support in acquiring the new task knowledge. 
                    In addition, by introducing a sensitivity measure to the loss of the current task from the associated tasks, we find cooperative relations between tasks while alleviating harmful interference. 
                    We apply the proposed approach to both task- and class-incremental scenarios in continual learning, using a wide range of datasets from small to large scales.
                    Experimental results show that the proposed method outperforms a large variety of continual learning approaches for the experiments while effectively alleviating catastrophic forgetting.
      - layout: left
        title: IEEE Access 2022
        sub_title: 
        link: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9695483
        link_text: [Link to Paper]
        caption: Impact Factor 3.367
        quote: >
           Gating Mechanism in Deep Neural Networks for Resource-Efficient Continual Learning, **Hyundong Jin**, Kimin Yun, and Eunwoo Kim 
        description: |
          Abstract: Catastrophic forgetting is a well-known tendency in continual learning of a deep neural network to forget previously learned knowledge when optimizing for sequentially incoming tasks. 
                    To address the issue, several methods have been proposed in research on continual learning. 
                    However, these methods cannot preserve the previously learned knowledge when training for a new task. 
                    Moreover, these methods are susceptible to negative interference between tasks, which may lead to catastrophic forgetting. 
                    It even becomes increasingly severe when there exists a notable gap between the domains of tasks. 
                    This paper proposes a novel method of controlling gates to select a subset of parameters learned for old tasks, which are then used to optimize a new task while avoiding negative interference efficiently. 
                    The proposed approach executes the subset of old parameters that provides positive responses by evaluating the effect when the old and new parameters are used together. 
                    The execution or skipping of old parameters through the gates is based on several responses across the network. 
                    We evaluate the proposed method in different continual learning scenarios involving image classification datasets. 
                    The proposed method outperforms other competitive methods and requires fewer parameters than the state-of-the-art methods during inference by applying the proposed gating mechanism that selectively involves a set of old parameters that provides positive prior knowledge to newer tasks. 
                    Additionally, we further prove the effectiveness of the proposed method through various analyses.
            
  - title: Invited Talk  # Title for the section
    layout: list # Type of content section (list/text)
    content:
      - layout: left
        title: Feb. 2023
        link: 
        # link_text: boringcompany.com
        sub_title: 
        caption: 
        quote: >
        description: | # this will include new lines to allow paragraphs
            2023 Korean Computer Vision Society (KCVS), Continual Learning session.
      - layout: left
        title: Dec. 2022
        link: 
        # link_text: boringcompany.com
        sub_title: 
        caption: 
        quote: >
        description: | # this will include new lines to allow paragraphs
            2022 Korean Artificial Intelligence Association (KAIA) and NAVER, CV / NLP session. 
# Footer
footer_show_references: true
# references_title: References on request (Override references text)

# Build settings
remote_theme: sproogen/resume-theme

sass:
  sass_dir: _sass
  style: compressed

plugins:
 - jekyll-seo-tag
